{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc6344c",
   "metadata": {},
   "source": [
    "# MedMNIST (DermaMNIST) — End-to-End KD Pipeline Notebook\n",
    "**Teacher**: ResNet-50  \n",
    "**Students**: ResNet-18, MobileNetV2, EfficientNet-B0  \n",
    "This notebook:\n",
    "- fetches DermaMNIST\n",
    "- trains teacher & students (with KD + optional Attention Transfer)\n",
    "- evaluates per-class metrics, confusions, PR curves\n",
    "- runs α/τ/β ablations, Grad-CAM, t-SNE\n",
    "- measures params, FLOPs, latency CPU/GPU, RAM peak\n",
    "- writes artifacts to `figs/`, `tables/`, and `reports/`\n",
    "- logs to TensorBoard\n",
    "It will **reuse existing checkpoints** under your `models/` tree when available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23bb6690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:\n",
      "  PROJECT_ROOT: .\n",
      "  REPORTS_ROOT: .\\reports\n",
      "  FIGS_ROOT: .\\figs\n",
      "  TABLES_ROOT: .\\tables\n",
      "  MODELS_ROOT: .\\models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# --- Notebook config|\n",
    "PROJECT_ROOT = \".\"  # set to your repo root if running elsewhere\n",
    "DATA_ROOT    = os.path.join(PROJECT_ROOT, \"MedMNIST-EdgeAI\", \"data\")  # optional; medmnist downloads will pick a cache too\n",
    "REPORTS_ROOT = os.path.join(PROJECT_ROOT, \"reports\")\n",
    "FIGS_ROOT    = os.path.join(PROJECT_ROOT, \"figs\")\n",
    "TABLES_ROOT  = os.path.join(PROJECT_ROOT, \"tables\")\n",
    "MODELS_ROOT  = os.path.join(PROJECT_ROOT, \"models\")\n",
    "\n",
    "os.makedirs(REPORTS_ROOT, exist_ok=True)\n",
    "os.makedirs(FIGS_ROOT, exist_ok=True)\n",
    "os.makedirs(TABLES_ROOT, exist_ok=True)\n",
    "\n",
    "print(\"Using:\")\n",
    "print(\"  PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"  REPORTS_ROOT:\", REPORTS_ROOT)\n",
    "print(\"  FIGS_ROOT:\", FIGS_ROOT)\n",
    "print(\"  TABLES_ROOT:\", TABLES_ROOT)\n",
    "print(\"  MODELS_ROOT:\", MODELS_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a637b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running in a fresh env, uncomment the next lines.\n",
    "# %pip install -q medmnist torch torchvision torchaudio scikit-learn pandas matplotlib tensorboard thop psutil\n",
    "# %pip install -q timm  # optional if using timm models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3feeea46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, time, math, random, psutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as tv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, average_precision_score, f1_score\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "try:\n",
    "    from thop import profile as thop_profile\n",
    "except Exception:\n",
    "    thop_profile = None\n",
    "\n",
    "SEED = 123\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE =\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e19b8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test: 7007 1003 2005 Classes: 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from medmnist import DermaMNIST\n",
    "\n",
    "def class_names_for(dataset):\n",
    "    if dataset == \"medmnist\":\n",
    "        return [\"akiec\",\"bcc\",\"bkl\",\"df\",\"mel\",\"nv\",\"vasc\"]\n",
    "    raise ValueError\n",
    "\n",
    "IMGSZ = 224\n",
    "TRANS_TRAIN = T.Compose([\n",
    "    T.Resize((IMGSZ,IMGSZ)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "TRANS_TEST = T.Compose([\n",
    "    T.Resize((IMGSZ,IMGSZ)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "trainset = DermaMNIST(split='train', transform=TRANS_TRAIN, download=True)\n",
    "valset   = DermaMNIST(split='val',   transform=TRANS_TEST,  download=True)\n",
    "testset  = DermaMNIST(split='test',  transform=TRANS_TEST,  download=True)\n",
    "\n",
    "C = len(class_names_for(\"medmnist\"))\n",
    "print(\"Train/Val/Test:\", len(trainset), len(valset), len(testset), \"Classes:\", C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687352b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_arch(tag: str, num_classes: int):\n",
    "    tag = tag.lower()\n",
    "    if tag == \"resnet50\":\n",
    "        m = tv.resnet50(weights=None)\n",
    "        m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "        return m\n",
    "    if tag == \"resnet18\":\n",
    "        m = tv.resnet18(weights=None)\n",
    "        m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "        return m\n",
    "    if tag in (\"mbv2\",\"mobilenet_v2\"):\n",
    "        m = tv.mobilenet_v2(weights=None)\n",
    "        m.classifier[-1] = nn.Linear(m.classifier[-1].in_features, num_classes)\n",
    "        return m\n",
    "    if tag in (\"effb0\",\"efficientnet_b0\"):\n",
    "        m = tv.efficientnet_b0(weights=None)\n",
    "        m.classifier[-1] = nn.Linear(m.classifier[-1].in_features, num_classes)\n",
    "        return m\n",
    "    raise ValueError(tag)\n",
    "\n",
    "def _load_state_smart(model, ckpt_path):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    sd = ckpt.get(\"state_dict\", ckpt if isinstance(ckpt, dict) else ckpt)\n",
    "    new_sd = {}\n",
    "    for k,v in sd.items():\n",
    "        k2 = k\n",
    "        for pref in [\"model.\",\"module.\",\"student.\",\"net.\"]:\n",
    "            if k2.startswith(pref): k2 = k2[len(pref):]\n",
    "        new_sd[k2] = v\n",
    "    miss, unexp = model.load_state_dict(new_sd, strict=False)\n",
    "    if miss:  print(\"[load_state] missing keys:\", len(miss))\n",
    "    if unexp: print(\"[load_state] unexpected keys:\", len(unexp))\n",
    "    return model\n",
    "\n",
    "def discover_student_ckpt(dataset, tag):\n",
    "    root = Path(MODELS_ROOT)/\"students\"\n",
    "    tmap = {\"resnet18\":\"resnet18\",\"mbv2\":\"mobilenetv2\",\"effb0\":\"efficientnetb0\"}\n",
    "    tagnorm = tmap[tag]\n",
    "    pats = [\n",
    "        root/f\"distilled_{tagnorm}_{dataset}/ckpt-best.pth\",\n",
    "        root/f\"*{dataset}*kdat*/ckpt-best.pth\",\n",
    "        root/\"**/ckpt-best.pth\",\n",
    "        root/\"**/ckpt-last.pth\"\n",
    "    ]\n",
    "    for p in pats:\n",
    "        hits = list(Path(root).glob(str(p.relative_to(root))))\n",
    "        if hits: return hits[0].as_posix()\n",
    "    return None\n",
    "\n",
    "def discover_teacher_ckpt(dataset):\n",
    "    root = Path(MODELS_ROOT)/\"teachers\"\n",
    "    pats = [\n",
    "        root/f\"{dataset}_resnet50*/ckpt-best.pth\",\n",
    "        root/\"**/ckpt-best.pth\",\n",
    "        root/\"**/ckpt-last.pth\"\n",
    "    ]\n",
    "    for p in pats:\n",
    "        hits = list(Path(root).glob(str(p.relative_to(root))))\n",
    "        if hits: return hits[0].as_posix()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "549eb43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ece_score_torch(probs, y, n_bins=15):\n",
    "    conf, preds = probs.max(1).values, probs.argmax(1)\n",
    "    bins = torch.linspace(0,1,n_bins+1, device=probs.device)\n",
    "    ece = torch.zeros(1, device=probs.device)\n",
    "    for i in range(n_bins):\n",
    "        m = (conf >= bins[i]) & (conf < bins[i+1])\n",
    "        if m.sum() == 0: continue\n",
    "        acc = (preds[m]==y[m]).float().mean()\n",
    "        conf_m = conf[m].mean()\n",
    "        ece += (m.float().mean()) * (conf_m - acc).abs()\n",
    "    return ece.item()\n",
    "\n",
    "def kd_loss(student_logits, teacher_logits, y, alpha, tau):\n",
    "    ce = F.cross_entropy(student_logits, y)\n",
    "    if alpha == 0.0: return ce\n",
    "    p = F.log_softmax(student_logits / tau, dim=1)\n",
    "    q = F.softmax(teacher_logits / tau, dim=1)\n",
    "    kl = F.kl_div(p, q, reduction=\"batchmean\") * (tau*tau)\n",
    "    return (1 - alpha) * ce + alpha * kl\n",
    "\n",
    "def at_loss(student_feat, teacher_feat):\n",
    "    def attn(x):\n",
    "        a = x.pow(2).mean(dim=1, keepdim=True)\n",
    "        a = a / (a.norm(p=2, dim=(2,3), keepdim=True) + 1e-8)\n",
    "        return a\n",
    "    return F.mse_loss(attn(student_feat), attn(teacher_feat))\n",
    "\n",
    "def pick_hook_layer(m):\n",
    "    if hasattr(m, \"layer4\"): return m.layer4[-1]\n",
    "    if hasattr(m, \"features\"): return list(m.features.children())[-1]\n",
    "    for mod in m.modules():\n",
    "        if isinstance(mod, nn.Conv2d): last = mod\n",
    "    return last\n",
    "\n",
    "def fit_supervised_teacher(dataset=\"medmnist\", epochs=8, batch=64, lr=3e-4):\n",
    "    classes = class_names_for(\"medmnist\")\n",
    "    C = len(classes)\n",
    "    dl_tr = DataLoader(trainset, batch_size=batch, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    dl_va = DataLoader(valset, batch_size=batch, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    m = build_arch(\"resnet50\", C).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(m.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    outdir = Path(MODELS_ROOT)/\"teachers\"/f\"medmnist_resnet50_sup\"\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir=str(outdir/\"tb\"))\n",
    "    best_f1 = -1.0; best_path = None\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        m.train()\n",
    "        for x,y in dl_tr:\n",
    "            x = x.to(DEVICE); y = y.long().to(DEVICE)\n",
    "            logits = m(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "\n",
    "        m.eval(); ys=[]; ps=[]\n",
    "        with torch.inference_mode():\n",
    "            for x,y in dl_va:\n",
    "                x = x.to(DEVICE); y = y.long().to(DEVICE)\n",
    "                prob = torch.softmax(m(x), dim=1).cpu()\n",
    "                ys.append(y.cpu()); ps.append(prob)\n",
    "        y_true = torch.cat(ys).numpy()\n",
    "        y_prob = torch.cat(ps).numpy()\n",
    "        y_pred = y_prob.argmax(1)\n",
    "        macf1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        ece = ece_score_torch(torch.from_numpy(y_prob), torch.from_numpy(y_true))\n",
    "        writer.add_scalar(\"val/macro_f1\", macf1, ep)\n",
    "        writer.add_scalar(\"val/ece\", ece, ep)\n",
    "\n",
    "        if macf1 > best_f1:\n",
    "            best_f1 = macf1\n",
    "            best_path = outdir/\"ckpt-best.pth\"\n",
    "            torch.save(m.state_dict(), best_path)\n",
    "        torch.save(m.state_dict(), outdir/\"ckpt-last.pth\")\n",
    "\n",
    "    writer.flush(); writer.close()\n",
    "    with open(outdir/\"metrics.json\",\"w\") as f:\n",
    "        json.dump({\"best_macro_f1\":float(best_f1)}, f)\n",
    "    return outdir.as_posix(), best_f1\n",
    "\n",
    "def fit_student_kd(student_tag=\"resnet18\", alpha=0.5, tau=4.0, beta=0.0, epochs=6, batch=64, lr=3e-4):\n",
    "    C = len(class_names_for(\"medmnist\"))\n",
    "    # teacher\n",
    "    t_ckpt = discover_teacher_ckpt(\"medmnist\")\n",
    "    if t_ckpt is None:\n",
    "        print(\"[train] No medmnist teacher ckpt found. Training a supervised teacher quickly...\")\n",
    "        t_dir, _ = fit_supervised_teacher(epochs=6, batch=batch)\n",
    "        t_ckpt = os.path.join(t_dir, \"ckpt-best.pth\")\n",
    "    t = build_arch(\"resnet50\", C).to(DEVICE)\n",
    "    t = _load_state_smart(t, t_ckpt).eval()\n",
    "\n",
    "    s = build_arch(student_tag, C).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(s.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    # feature hooks if beta>0\n",
    "    if beta > 0:\n",
    "        tl = pick_hook_layer(t); sl = pick_hook_layer(s)\n",
    "        feat_t, feat_s = {}, {}\n",
    "        tl.register_forward_hook(lambda m,i,o: feat_t.setdefault(\"z\", o))\n",
    "        sl.register_forward_hook(lambda m,i,o: feat_s.setdefault(\"z\", o))\n",
    "\n",
    "    dl_tr = DataLoader(trainset, batch_size=batch, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    dl_va = DataLoader(valset, batch_size=batch, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    tag = f\"KD_a{alpha}_t{tau}_b{beta}_{student_tag}_medmnist\"\n",
    "    outdir = Path(REPORTS_ROOT)/\"ablation_medmnist\"/tag\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir=str(outdir/\"tb\"))\n",
    "    best_f1 = -1.0\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        s.train()\n",
    "        for x,y in dl_tr:\n",
    "            x = x.to(DEVICE); y = y.long().to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                t_logits = t(x)\n",
    "            s_logits = s(x)\n",
    "            loss = kd_loss(s_logits, t_logits, y, alpha, tau)\n",
    "            if beta > 0:\n",
    "                loss = loss + beta * at_loss(feat_s[\"z\"], feat_t[\"z\"])\n",
    "                feat_t.clear(); feat_s.clear()\n",
    "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "\n",
    "        # val\n",
    "        s.eval(); ys=[]; ps=[]\n",
    "        with torch.inference_mode():\n",
    "            for x,y in dl_va:\n",
    "                x = x.to(DEVICE); y = y.long().to(DEVICE)\n",
    "                prob = torch.softmax(s(x), dim=1).cpu()\n",
    "                ys.append(y.cpu()); ps.append(prob)\n",
    "        y_true = torch.cat(ys).numpy()\n",
    "        y_prob = torch.cat(ps).numpy()\n",
    "        y_pred = y_prob.argmax(1)\n",
    "        macf1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        ece = ece_score_torch(torch.from_numpy(y_prob), torch.from_numpy(y_true))\n",
    "        writer.add_scalar(\"val/macro_f1\", macf1, ep)\n",
    "        writer.add_scalar(\"val/ece\", ece, ep)\n",
    "\n",
    "        rec = {\"epoch\": ep, \"macro_f1\": float(macf1), \"ece\": float(ece),\n",
    "               \"alpha\": float(alpha), \"tau\": float(tau), \"beta\": float(beta),\n",
    "               \"student\": student_tag, \"dataset\": \"medmnist\"}\n",
    "        with open(outdir/\"metrics.jsonl\",\"a\") as f: f.write(json.dumps(rec)+\"\\n\")\n",
    "\n",
    "        if macf1 > best_f1:\n",
    "            best_f1 = macf1\n",
    "            torch.save(s.state_dict(), outdir/\"model_best.pth\")\n",
    "        torch.save(s.state_dict(), outdir/\"model_last.pth\")\n",
    "\n",
    "    writer.flush(); writer.close()\n",
    "    return outdir.as_posix(), best_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e4db6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_and_save(model, loader, classes, out_tab_dir, out_fig_dir, tag):\n",
    "    os.makedirs(out_tab_dir, exist_ok=True)\n",
    "    os.makedirs(out_fig_dir, exist_ok=True)\n",
    "    ys=[]; ps=[]\n",
    "    with torch.inference_mode():\n",
    "        for x,y in loader:\n",
    "            x = x.to(DEVICE); y = y.long().to(DEVICE)\n",
    "            p = torch.softmax(model(x), dim=1).cpu()\n",
    "            ys.append(y.cpu()); ps.append(p)\n",
    "    y_true = torch.cat(ys).numpy()\n",
    "    y_prob = torch.cat(ps).numpy()\n",
    "    y_pred = y_prob.argmax(1)\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=classes, output_dict=True, zero_division=0)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv(os.path.join(out_tab_dir, f\"{tag}_perclass_metrics.csv\"))\n",
    "    try:\n",
    "        df.to_latex(os.path.join(out_tab_dir, f\"{tag}_perclass_metrics.tex\"), float_format=\"%.3f\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    fig = plt.figure(figsize=(7,6)); plt.imshow(cm, interpolation='nearest'); plt.title(f\"Confusion: {tag}\")\n",
    "    plt.colorbar(); plt.xticks(range(len(classes)), classes, rotation=45, ha=\"right\"); plt.yticks(range(len(classes)), classes)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]): plt.text(j,i,str(cm[i,j]),ha=\"center\",va=\"center\")\n",
    "    plt.tight_layout(); fig.savefig(os.path.join(out_fig_dir, f\"{tag}_confmat.png\"), dpi=200); plt.close(fig)\n",
    "\n",
    "    # PR curves\n",
    "    fig = plt.figure(figsize=(7,6))\n",
    "    for c in range(len(classes)):\n",
    "        prec, rec, _ = precision_recall_curve((y_true==c).astype(int), y_prob[:,c])\n",
    "        ap = average_precision_score((y_true==c).astype(int), y_prob[:,c])\n",
    "        plt.plot(rec, prec, label=f\"{classes[c]} (AP={ap:.3f})\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.legend(); plt.title(f\"PR Curves: {tag}\")\n",
    "    fig.savefig(os.path.join(out_fig_dir, f\"{tag}_pr_curves.png\"), dpi=200); plt.close(fig)\n",
    "\n",
    "    return y_true, y_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d644a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_stats(model, imgsz=IMGSZ, device=DEVICE, reps=20, warmup=10):\n",
    "    x = torch.randn(1,3,imgsz,imgsz).to(device)\n",
    "    # Params\n",
    "    params_m = sum(p.numel() for p in model.parameters())/1e6\n",
    "\n",
    "    # FLOPs if thop available\n",
    "    flops_g = float(\"nan\")\n",
    "    if thop_profile is not None:\n",
    "        try:\n",
    "            macs, _ = thop_profile(model, inputs=(x,), verbose=False)\n",
    "            flops_g = macs/1e9\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Latency CPU/GPU\n",
    "    def bench_on(d):\n",
    "        xm = x.to(d); model.to(d).eval()\n",
    "        times=[]\n",
    "        if d==\"cuda\":\n",
    "            for _ in range(warmup):\n",
    "                with torch.inference_mode():\n",
    "                    _ = model(xm); torch.cuda.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            for _ in range(warmup):\n",
    "                with torch.inference_mode():\n",
    "                    _ = model(xm)\n",
    "        for _ in range(reps):\n",
    "            t0=time.time()\n",
    "            with torch.inference_mode():\n",
    "                _ = model(xm)\n",
    "                if d==\"cuda\": torch.cuda.synchronize()\n",
    "            times.append((time.time()-t0)*1000.0)\n",
    "        return np.mean(times), np.std(times)\n",
    "\n",
    "    lat_gpu, std_gpu = (float(\"nan\"), float(\"nan\"))\n",
    "    if torch.cuda.is_available():\n",
    "        lat_gpu, std_gpu = bench_on(\"cuda\")\n",
    "    lat_cpu, std_cpu = bench_on(\"cpu\")\n",
    "\n",
    "    # RAM peak (process RSS); GPU mem peak if CUDA\n",
    "    rss_mb = psutil.Process(os.getpid()).memory_info().rss/1024/1024\n",
    "    gpu_peak_mb = float(\"nan\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        with torch.inference_mode():\n",
    "            _ = model(x.to(\"cuda\"))\n",
    "            torch.cuda.synchronize()\n",
    "        gpu_peak_mb = torch.cuda.max_memory_allocated()/1024/1024\n",
    "\n",
    "    return {\n",
    "        \"params_M\": params_m, \"FLOPs_G\": flops_g,\n",
    "        \"latency_ms_gpu_mean\": lat_gpu, \"latency_ms_gpu_std\": std_gpu,\n",
    "        \"latency_ms_cpu_mean\": lat_cpu, \"latency_ms_cpu_std\": std_cpu,\n",
    "        \"ram_rss_MB\": rss_mb, \"gpu_peak_MB\": gpu_peak_mb\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e6c7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pick_last_layer(model: nn.Module):\n",
    "    if hasattr(model, \"layer4\"): return model.layer4[-1]\n",
    "    if hasattr(model, \"features\"): return list(model.features.children())[-1]\n",
    "    for mod in model.modules():\n",
    "        if isinstance(mod, nn.Conv2d): last = mod\n",
    "    return last\n",
    "\n",
    "def gradcam(model, x4d, target_layer, class_idx):\n",
    "    model.eval()\n",
    "    feats, grads = {}, {}\n",
    "    def fwd_hook(m,i,o): feats[\"z\"]=o\n",
    "    def bwd_hook(m,gi,go): grads[\"g\"]=go[0]\n",
    "    h1 = target_layer.register_forward_hook(fwd_hook)\n",
    "    h2 = target_layer.register_full_backward_hook(bwd_hook)\n",
    "    logits = model(x4d); score = logits[0,class_idx]\n",
    "    model.zero_grad(set_to_none=True); score.backward(retain_graph=True)\n",
    "    A = feats[\"z\"].detach(); G = grads[\"g\"].detach()\n",
    "    w = G.flatten(2).mean(2)  # [1,K]\n",
    "    cam = (w[:,:,None,None]*A).sum(1)\n",
    "    cam = torch.relu(cam)[0].cpu().numpy()\n",
    "    cam = (cam - cam.min())/(cam.max()+1e-6)\n",
    "    h1.remove(); h2.remove()\n",
    "    return cam\n",
    "\n",
    "def tsne_features(model, loader, k=1500):\n",
    "    model.eval()\n",
    "    X=[]; yall=[]\n",
    "    with torch.inference_mode():\n",
    "        for i,(x,y) in enumerate(loader):\n",
    "            x = x.to(DEVICE)\n",
    "            # penultimate\n",
    "            if hasattr(model, \"layer4\"):\n",
    "                z = model.conv1(x); z = model.bn1(z); z = model.relu(z); z = model.maxpool(z)\n",
    "                z = model.layer1(z); z = model.layer2(z); z = model.layer3(z); z = model.layer4(z)\n",
    "                z = F.adaptive_avg_pool2d(z, 1).flatten(1)\n",
    "            else:\n",
    "                z = model.features(x)\n",
    "                z = F.adaptive_avg_pool2d(z, 1).flatten(1)\n",
    "            X.append(z.cpu().numpy()); yall.append(y.numpy())\n",
    "            if len(np.concatenate(yall)) >= k: break\n",
    "    X = np.vstack(X); y = np.concatenate(yall)[:k]\n",
    "    X = X[:k]\n",
    "    ts = TSNE(n_components=2, perplexity=30, init=\"pca\", learning_rate=\"auto\", random_state=0).fit_transform(X)\n",
    "    return ts, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a51aee",
   "metadata": {},
   "source": [
    "## Train / Load Teacher (ResNet-50) for DermaMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b6d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load] resnet50 <- MedMNIST-EdgeAI/models/resnet50_teacher_dermamnist.pth\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Load] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00march_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m <- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state_flexible(m, path)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m teacher = \u001b[43mload_user_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresnet50\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m student_resnet18 = load_user_model(\u001b[33m\"\u001b[39m\u001b[33mresnet18\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m student_mbv2 = load_user_model(\u001b[33m\"\u001b[39m\u001b[33mmobilenet_v2\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mload_user_model\u001b[39m\u001b[34m(arch_tag)\u001b[39m\n\u001b[32m     35\u001b[39m m = build_arch(arch_tag, num_classes).to(DEVICE).eval()\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Load] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00march_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m <- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_state_flexible\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36m_load_state_flexible\u001b[39m\u001b[34m(model, path)\u001b[39m\n\u001b[32m     22\u001b[39m new_sd = {}\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m state.items():\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     nk = \u001b[43mre\u001b[49m.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m^(module\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m.|model\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m.|student\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m.|net\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m.)\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, k)\n\u001b[32m     25\u001b[39m     new_sd[nk] = v\n\u001b[32m     26\u001b[39m missing, unexpected = model.load_state_dict(new_sd, strict=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Load teacher & students from user-provided checkpoints (no training) ===\n",
    "import torch, os\n",
    "from pathlib import Path\n",
    "import re\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_classes = 7  # DermaMNIST\n",
    "\n",
    "# Exact file mapping provided by user\n",
    "ckpt_map = {\n",
    "    \"resnet50\":      \"MedMNIST-EdgeAI/models/resnet50_teacher_dermamnist.pth\",\n",
    "    \"resnet18\":      \"MedMNIST-EdgeAI/models/resnet18/resnet18_dermamnist_student.pth\",\n",
    "    \"mobilenet_v2\":  \"MedMNIST-EdgeAI/models/mobilenet_v2/mobilenet_v2_dermamnist_student.pth\",\n",
    "    \"efficientnet_b0\":\"MedMNIST-EdgeAI/models/efficientnet_b0/efficientnet_b0_dermamnist_student.pth\",\n",
    "}\n",
    "\n",
    "# Reuse the in-notebook arch builder\n",
    "# (build_arch was defined earlier in the notebook)\n",
    "def _load_state_flexible(model, path):\n",
    "    state = torch.load(path, map_location=\"cpu\")\n",
    "    if isinstance(state, dict) and \"state_dict\" in state and isinstance(state[\"state_dict\"], dict):\n",
    "        state = state[\"state_dict\"]\n",
    "    new_sd = {}\n",
    "    for k, v in state.items():\n",
    "        nk = re.sub(r\"^(module\\.|model\\.|student\\.|net\\.)\", \"\", k)\n",
    "        new_sd[nk] = v\n",
    "    missing, unexpected = model.load_state_dict(new_sd, strict=False)\n",
    "    print(f\"[state] {os.path.basename(path)} missing={len(missing)} unexpected={len(unexpected)}\")\n",
    "    return model\n",
    "\n",
    "def load_user_model(arch_tag):\n",
    "    assert arch_tag in ckpt_map, f\"No ckpt mapping for {arch_tag}\"\n",
    "    path = ckpt_map[arch_tag]\n",
    "    if not Path(path).exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {path}\")\n",
    "    m = build_arch(arch_tag, num_classes).to(DEVICE).eval()\n",
    "    print(f\"[Load] {arch_tag} <- {path}\")\n",
    "    return _load_state_flexible(m, path)\n",
    "\n",
    "teacher = load_user_model(\"resnet50\")\n",
    "student_resnet18 = load_user_model(\"resnet18\")\n",
    "student_mbv2 = load_user_model(\"mobilenet_v2\")\n",
    "student_effb0 = load_user_model(\"efficientnet_b0\")\n",
    "\n",
    "# expose a dictionary for later cells\n",
    "models_for_eval = {\n",
    "    \"resnet50\": teacher,\n",
    "    \"resnet18\": student_resnet18,\n",
    "    \"mobilenet_v2\": student_mbv2,\n",
    "    \"efficientnet_b0\": student_effb0,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7005ad",
   "metadata": {},
   "source": [
    "## Evaluate Teacher and Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58019d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classes = class_names_for(\"medmnist\")\n",
    "test_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
    "tab_dir = os.path.join(TABLES_ROOT, \"medmnist\"); fig_dir = os.path.join(FIGS_ROOT, \"medmnist\")\n",
    "y_true_T, y_prob_T = eval_and_save(teacher, test_loader, classes, tab_dir, fig_dir, tag=\"resnet50\")\n",
    "print(\"Teacher eval saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d1a1cc",
   "metadata": {},
   "source": [
    "## Train / Load Students with KD (quick sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed7ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Register ablation records from provided checkpoints (single-point runs) ===\n",
    "# We write one jsonl per 'run' so downstream ablation aggregator can read.\n",
    "import json, os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "REPORTS_ROOT = REPORTS_ROOT if 'REPORTS_ROOT' in globals() else \"./reports\"\n",
    "ABL_DIR = Path(REPORTS_ROOT) / \"ablation_medmnist\"\n",
    "\n",
    "classes = class_names_for(\"medmnist\")\n",
    "test_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "def ece_score_torch(probs, y, n_bins=15):\n",
    "    conf, preds = probs.max(1).values, probs.argmax(1)\n",
    "    bins = torch.linspace(0,1,n_bins+1, device=probs.device)\n",
    "    ece = torch.zeros(1, device=probs.device)\n",
    "    for i in range(n_bins):\n",
    "        m = (conf >= bins[i]) & (conf < bins[i+1])\n",
    "        if m.sum() == 0: continue\n",
    "        acc = (preds[m]==y[m]).float().mean()\n",
    "        conf_m = conf[m].mean()\n",
    "        ece += (m.float().mean()) * (conf_m - acc).abs()\n",
    "    return ece.item()\n",
    "\n",
    "# Evaluate each provided student vs test set, write a single ablation row:\n",
    "grid_defaults = {\n",
    "    \"resnet18\":      {\"alpha\":0.5,\"tau\":4.0,\"beta\":0.0},\n",
    "    \"mobilenet_v2\":  {\"alpha\":0.5,\"tau\":4.0,\"beta\":0.0},\n",
    "    \"efficientnet_b0\":{\"alpha\":0.5,\"tau\":4.0,\"beta\":0.0},\n",
    "}\n",
    "\n",
    "best_dirs = {}  # to keep interface stable for later cells\n",
    "for st_tag, hp in grid_defaults.items():\n",
    "    m = models_for_eval[st_tag]\n",
    "    # eval\n",
    "    ys=[]; ps=[]\n",
    "    with torch.inference_mode():\n",
    "        for x,y in test_loader:\n",
    "            x = x.to(DEVICE); y = y.long().to(DEVICE)\n",
    "            p = torch.softmax(m(x), dim=1).cpu()\n",
    "            ys.append(y.cpu()); ps.append(p)\n",
    "    y_true = torch.cat(ys).numpy()\n",
    "    y_prob = torch.cat(ps).numpy()\n",
    "    y_pred = y_prob.argmax(1)\n",
    "    macf1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    ece = ece_score_torch(torch.from_numpy(y_prob), torch.from_numpy(y_true))\n",
    "    # write a jsonl\n",
    "    outdir = ABL_DIR / f\"fixed_{st_tag}_medmnist\"\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    rec = {\"epoch\": 0, \"macro_f1\": float(macf1), \"ece\": float(ece),\n",
    "           \"alpha\": float(hp[\"alpha\"]), \"tau\": float(hp[\"tau\"]), \"beta\": float(hp[\"beta\"]),\n",
    "           \"student\": st_tag, \"dataset\": \"medmnist\"}\n",
    "    with open(outdir/\"metrics.jsonl\",\"w\") as f: f.write(json.dumps(rec)+\"\\n\")\n",
    "    best_dirs[(st_tag,hp[\"alpha\"],hp[\"tau\"],hp[\"beta\"])] = outdir.as_posix()\n",
    "\n",
    "print(\"[OK] Wrote single-point ablation rows for provided student checkpoints.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911e1cfc",
   "metadata": {},
   "source": [
    "## Evaluate Students and Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b44cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate already-loaded models and save artifacts\n",
    "classes = class_names_for(\"medmnist\")\n",
    "test_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
    "for tag, m in models_for_eval.items():\n",
    "    tab_dir = os.path.join(TABLES_ROOT, \"medmnist\")\n",
    "    fig_dir = os.path.join(FIGS_ROOT, \"medmnist\")\n",
    "    eval_and_save(m, test_loader, classes, tab_dir, fig_dir, tag=tag)\n",
    "print(\"Student/Teacher eval saved from provided checkpoints.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ea37f",
   "metadata": {},
   "source": [
    "## Efficiency: Params / FLOPs / Latency / RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62ed4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = []\n",
    "for tag, m in models_for_eval.items():\n",
    "    stat = model_stats(m)\n",
    "    stat[\"model\"] = tag\n",
    "    rows.append(stat)\n",
    "\n",
    "df_eff = pd.DataFrame(rows)[[\"model\",\"params_M\",\"FLOPs_G\",\"latency_ms_gpu_mean\",\"latency_ms_cpu_mean\",\"ram_rss_MB\",\"gpu_peak_MB\"]]\n",
    "df_eff.to_csv(os.path.join(TABLES_ROOT, \"efficiency_medmnist.csv\"), index=False)\n",
    "try:\n",
    "    df_eff.to_latex(os.path.join(TABLES_ROOT, \"efficiency_medmnist.tex\"), float_format=\"%.3f\", index=False)\n",
    "except Exception:\n",
    "    pass\n",
    "df_eff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f6190b",
   "metadata": {},
   "source": [
    "## Grad-CAM Panels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def overlay(rgb_u8, cam, alpha=0.35):\n",
    "    import cv2\n",
    "    H,W = rgb_u8.shape[:2]\n",
    "    cam_up = cv2.resize(cam, (W,H), interpolation=cv2.INTER_LINEAR)\n",
    "    heat = cv2.applyColorMap(np.uint8(255*cam_up), cv2.COLORMAP_JET)\n",
    "    heat = cv2.cvtColor(heat, cv2.COLOR_BGR2RGB)\n",
    "    out = (1-alpha)*rgb_u8 + alpha*heat\n",
    "    return np.clip(out,0,255).astype(np.uint8)\n",
    "\n",
    "# take K examples per class for panel\n",
    "K = 10\n",
    "idxs_per_class = {c:[] for c in range(len(classes))}\n",
    "for i in range(len(testset)):\n",
    "    _, y = testset[i]\n",
    "    y = int(np.array(y).squeeze())\n",
    "    if 0 <= y < len(classes) and len(idxs_per_class[y]) < K:\n",
    "        idxs_per_class[y].append(i)\n",
    "    if all(len(v)>=K for v in idxs_per_class.values()): break\n",
    "\n",
    "import cv2\n",
    "for tag, m in models_for_eval.items():\n",
    "    tl = pick_last_layer(m)\n",
    "    rows=[]\n",
    "    for c in range(len(classes)):\n",
    "        tiles=[]\n",
    "        for i in idxs_per_class[c]:\n",
    "            x, y = testset[i]\n",
    "            x = x.unsqueeze(0).to(DEVICE)\n",
    "            cam = gradcam(m, x, tl, c)\n",
    "            # de-normalize for viz\n",
    "            mean = torch.tensor([0.485,0.456,0.406])[:,None,None]\n",
    "            std  = torch.tensor([0.229,0.224,0.225])[:,None,None]\n",
    "            viz = (x[0].cpu()*std + mean).clamp(0,1).permute(1,2,0).numpy()\n",
    "            viz = (viz*255).astype(np.uint8)\n",
    "            tiles.append(overlay(viz, cam))\n",
    "        rows.append(np.concatenate(tiles, axis=1))\n",
    "    panel = np.concatenate(rows, axis=0)\n",
    "    outp = os.path.join(FIGS_ROOT, \"medmnist\", f\"{tag}_gradcam_panel.png\")\n",
    "    os.makedirs(os.path.dirname(outp), exist_ok=True)\n",
    "    cv2.imwrite(outp, cv2.cvtColor(panel, cv2.COLOR_RGB2BGR))\n",
    "outp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70142e31",
   "metadata": {},
   "source": [
    "## t-SNE: Teacher vs Students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35df38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def orthogonal_procrustes(A, B, scale=True):\n",
    "    A0 = A - A.mean(0, keepdims=True)\n",
    "    B0 = B - B.mean(0, keepdims=True)\n",
    "    if scale:\n",
    "        sA = np.sqrt((A0**2).sum()); sB = np.sqrt((B0**2).sum())\n",
    "        A0 = A0/(sA+1e-12); B0 = B0/(sB+1e-12)\n",
    "    U, _, Vt = np.linalg.svd(A0.T @ B0)\n",
    "    R = U @ Vt\n",
    "    Ahat = A0 @ R\n",
    "    if scale: Ahat *= (sB/(sA+1e-12))\n",
    "    Ahat += B.mean(0, keepdims=True)\n",
    "    return Ahat\n",
    "\n",
    "loader_small = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n",
    "T2, y = tsne_features(teacher, loader_small, k=1500)\n",
    "\n",
    "for st in students:\n",
    "    # use loaded student instance for efficiency table if available\n",
    "    m = models_for_eval.get(st, None)\n",
    "    if m is None:\n",
    "        # fallback: load best from sweep dirs\n",
    "        cand = sorted([d for k,d in best_dirs.items() if k[0]==st], reverse=True)\n",
    "        if not cand: continue\n",
    "        ck = os.path.join(cand[0], \"model_best.pth\")\n",
    "        m = build_arch(st, len(classes)).to(DEVICE); m = _load_state_smart(m, ck).eval()\n",
    "    S2, _ = tsne_features(m, loader_small, k=1500)\n",
    "    S2a = orthogonal_procrustes(S2, T2, scale=True)\n",
    "\n",
    "    fig=plt.figure(figsize=(6,5))\n",
    "    plt.scatter(T2[:,0], T2[:,1], s=6, alpha=0.35, label=\"resnet50\")\n",
    "    plt.scatter(S2a[:,0], S2a[:,1], s=6, alpha=0.35, label=st)\n",
    "    plt.title(f\"t-SNE MedMNIST — resnet50 vs {st}\")\n",
    "    plt.legend()\n",
    "    outp = os.path.join(FIGS_ROOT, \"medmnist\", f\"tsne_resnet50_vs_{st}.png\")\n",
    "    plt.savefig(outp, dpi=200, bbox_inches=\"tight\"); plt.close(fig)\n",
    "outp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62a7c66",
   "metadata": {},
   "source": [
    "## Aggregate Ablations (α, τ, β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe55b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "abl_dir = Path(REPORTS_ROOT)/\"ablation_medmnist\"\n",
    "rows=[]\n",
    "for fp in abl_dir.rglob(\"metrics.jsonl\"):\n",
    "    with open(fp,\"r\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip(): continue\n",
    "            rows.append(json.loads(line))\n",
    "df = pd.DataFrame(rows)\n",
    "if df.empty:\n",
    "    print(\"[warn] No ablation rows found in\", abl_dir)\n",
    "else:\n",
    "    df_best = df.sort_values(\"macro_f1\").drop_duplicates(subset=[\"student\",\"alpha\",\"tau\",\"beta\"], keep=\"last\")\n",
    "    out_tab_dir = os.path.join(TABLES_ROOT, \"medmnist\")\n",
    "    out_fig_dir = os.path.join(FIGS_ROOT, \"medmnist\")\n",
    "    os.makedirs(out_tab_dir, exist_ok=True); os.makedirs(out_fig_dir, exist_ok=True)\n",
    "    df_best.to_csv(os.path.join(out_tab_dir,\"ablation_grid.csv\"), index=False)\n",
    "    try:\n",
    "        df_best.to_latex(os.path.join(out_tab_dir,\"ablation_grid.tex\"), float_format=\"%.3f\", index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    for stu, g in df_best.groupby(\"student\"):\n",
    "        g2 = g.sort_values(\"macro_f1\").drop_duplicates(subset=[\"alpha\",\"tau\"], keep=\"last\")\n",
    "        piv = g2.pivot_table(index=\"alpha\", columns=\"tau\", values=\"macro_f1\", aggfunc=\"max\")\n",
    "        fig=plt.figure(figsize=(6,5)); plt.imshow(piv.values, aspect=\"auto\")\n",
    "        plt.title(f\"MedMNIST — {stu} (Macro-F1)\"); plt.xlabel(\"tau\"); plt.ylabel(\"alpha\")\n",
    "        plt.xticks(range(len(piv.columns)), [str(c) for c in piv.columns])\n",
    "        plt.yticks(range(len(piv.index)), [str(c) for c in piv.index])\n",
    "        plt.colorbar(label=\"Macro-F1\")\n",
    "        plt.savefig(os.path.join(out_fig_dir, f\"ablation_heatmap_alpha_tau_{stu}.png\"), dpi=200, bbox_inches=\"tight\"); plt.close(fig)\n",
    "    print(\"Ablation tables/figures saved.\")\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
