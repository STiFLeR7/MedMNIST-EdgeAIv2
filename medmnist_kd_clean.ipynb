{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20440efa",
   "metadata": {},
   "source": [
    "# MedMNIST DermaMNIST — KD Evaluation (Pretrained Checkpoints)\n",
    "This notebook loads your **pretrained** MedMNIST checkpoints for teacher (ResNet-50) and students (ResNet-18, MobileNetV2, EfficientNet-B0), evaluates them, logs a single-point ablation record per student, computes efficiency stats, and produces Grad-CAM & t-SNE visualizations.\n",
    "\n",
    "**It does not train.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "799860ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPORTS_ROOT: .\\reports\n",
      "FIGS_ROOT   : .\\figs\\medmnist\n",
      "TABLES_ROOT : .\\tables\\medmnist\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Paths & configuration ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = \".\"\n",
    "REPORTS_ROOT = os.path.join(PROJECT_ROOT, \"reports\")\n",
    "FIGS_ROOT    = os.path.join(PROJECT_ROOT, \"figs\", \"medmnist\")\n",
    "TABLES_ROOT  = os.path.join(PROJECT_ROOT, \"tables\", \"medmnist\")\n",
    "\n",
    "# User-provided pretrained checkpoints\n",
    "CKPT_MAP = {\n",
    "    \"resnet50\":       \"MedMNIST-EdgeAI/models/resnet50_teacher_dermamnist.pth\",\n",
    "    \"resnet18\":       \"MedMNIST-EdgeAI/models/resnet18/resnet18_dermamnist_student.pth\",\n",
    "    \"mobilenet_v2\":   \"MedMNIST-EdgeAI/models/mobilenet_v2/mobilenet_v2_dermamnist_student.pth\",\n",
    "    \"efficientnet_b0\":\"MedMNIST-EdgeAI/models/efficientnet_b0/efficientnet_b0_dermamnist_student.pth\",\n",
    "}\n",
    "\n",
    "os.makedirs(REPORTS_ROOT, exist_ok=True)\n",
    "os.makedirs(FIGS_ROOT, exist_ok=True)\n",
    "os.makedirs(TABLES_ROOT, exist_ok=True)\n",
    "\n",
    "print(\"REPORTS_ROOT:\", REPORTS_ROOT)\n",
    "print(\"FIGS_ROOT   :\", FIGS_ROOT)\n",
    "print(\"TABLES_ROOT :\", TABLES_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6dc2ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Dependencies ---\n",
    "# If running in a fresh env, you may need:\n",
    "# %pip install -q medmnist torch torchvision scikit-learn pandas matplotlib thop psutil\n",
    "\n",
    "import math, time, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as tv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                             precision_recall_curve, average_precision_score, f1_score)\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "try:\n",
    "    from thop import profile as thop_profile\n",
    "except Exception:\n",
    "    thop_profile = None\n",
    "\n",
    "import psutil\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 123\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "print(\"DEVICE:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e80b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test: 7007 1003 2005 Classes: 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Dataset: DermaMNIST ---\n",
    "from medmnist import DermaMNIST\n",
    "\n",
    "def class_names_for_medmnist():\n",
    "    return [\"akiec\",\"bcc\",\"bkl\",\"df\",\"mel\",\"nv\",\"vasc\"]\n",
    "\n",
    "IMGSZ = 224\n",
    "TRANS_TRAIN = T.Compose([\n",
    "    T.Resize((IMGSZ,IMGSZ)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "TRANS_TEST = T.Compose([\n",
    "    T.Resize((IMGSZ,IMGSZ)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "trainset = DermaMNIST(split='train', transform=TRANS_TRAIN, download=True)\n",
    "valset   = DermaMNIST(split='val',   transform=TRANS_TEST,  download=True)\n",
    "testset  = DermaMNIST(split='test',  transform=TRANS_TEST,  download=True)\n",
    "\n",
    "C = len(class_names_for_medmnist())\n",
    "print(\"Train/Val/Test:\", len(trainset), len(valset), len(testset), \"Classes:\", C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4bdcc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Model builders and checkpoint loader ---\n",
    "def build_arch(tag: str, num_classes: int):\n",
    "    tag = tag.lower()\n",
    "    if tag == \"resnet50\":\n",
    "        m = tv.resnet50(weights=None)\n",
    "        m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "        return m\n",
    "    if tag == \"resnet18\":\n",
    "        m = tv.resnet18(weights=None)\n",
    "        m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "        return m\n",
    "    if tag in (\"mobilenet_v2\",\"mbv2\"):\n",
    "        m = tv.mobilenet_v2(weights=None)\n",
    "        m.classifier[-1] = nn.Linear(m.classifier[-1].in_features, num_classes)\n",
    "        return m\n",
    "    if tag in (\"efficientnet_b0\",\"effb0\"):\n",
    "        m = tv.efficientnet_b0(weights=None)\n",
    "        m.classifier[-1] = nn.Linear(m.classifier[-1].in_features, num_classes)\n",
    "        return m\n",
    "    raise ValueError(f\"Unknown arch: {tag}\")\n",
    "\n",
    "import re\n",
    "\n",
    "def load_checkpoint_flexible(model: nn.Module, ckpt_path: str):\n",
    "    state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    if isinstance(state, dict) and \"state_dict\" in state and isinstance(state[\"state_dict\"], dict):\n",
    "        state = state[\"state_dict\"]\n",
    "    new_sd = {}\n",
    "    for k,v in state.items():\n",
    "        nk = re.sub(r\"^(module\\.|model\\.|student\\.|net\\.)\", \"\", k)\n",
    "        new_sd[nk] = v\n",
    "    missing, unexpected = model.load_state_dict(new_sd, strict=False)\n",
    "    print(f\"[load] {os.path.basename(ckpt_path)}  missing={len(missing)}  unexpected={len(unexpected)}\")\n",
    "    return model\n",
    "\n",
    "def load_user_model(arch_tag: str, ckpt_map: dict, num_classes: int, device: str):\n",
    "    assert arch_tag in ckpt_map, f\"No checkpoint mapping for {arch_tag}\"\n",
    "    path = ckpt_map[arch_tag]\n",
    "    if not Path(path).exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {path}\")\n",
    "    m = build_arch(arch_tag, num_classes).to(device).eval()\n",
    "    print(f\"[Load] {arch_tag} <- {path}\")\n",
    "    return load_checkpoint_flexible(m, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49d2fcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load] resnet50 <- MedMNIST-EdgeAI/models/resnet50_teacher_dermamnist.pth\n",
      "[load] resnet50_teacher_dermamnist.pth  missing=0  unexpected=0\n",
      "[Load] resnet18 <- MedMNIST-EdgeAI/models/resnet18/resnet18_dermamnist_student.pth\n",
      "[load] resnet18_dermamnist_student.pth  missing=0  unexpected=0\n",
      "[Load] mobilenet_v2 <- MedMNIST-EdgeAI/models/mobilenet_v2/mobilenet_v2_dermamnist_student.pth\n",
      "[load] mobilenet_v2_dermamnist_student.pth  missing=0  unexpected=0\n",
      "[Load] efficientnet_b0 <- MedMNIST-EdgeAI/models/efficientnet_b0/efficientnet_b0_dermamnist_student.pth\n",
      "[load] efficientnet_b0_dermamnist_student.pth  missing=0  unexpected=0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Load teacher and students ---\n",
    "num_classes = 7\n",
    "teacher = load_user_model(\"resnet50\", CKPT_MAP, num_classes, DEVICE)\n",
    "student_resnet18 = load_user_model(\"resnet18\", CKPT_MAP, num_classes, DEVICE)\n",
    "student_mbv2 = load_user_model(\"mobilenet_v2\", CKPT_MAP, num_classes, DEVICE)\n",
    "student_effb0 = load_user_model(\"efficientnet_b0\", CKPT_MAP, num_classes, DEVICE)\n",
    "\n",
    "MODELS = {\n",
    "    \"resnet50\": teacher,\n",
    "    \"resnet18\": student_resnet18,\n",
    "    \"mobilenet_v2\": student_mbv2,\n",
    "    \"efficientnet_b0\": student_effb0,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae86c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Evaluation helpers ---\n",
    "def eval_model(model: nn.Module, loader: DataLoader, classes: list):\n",
    "    y_true, y_prob = [], []\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for x,y in loader:\n",
    "            x = x.to(DEVICE); y = y.long().to(DEVICE)\n",
    "            p = torch.softmax(model(x), dim=1).cpu().numpy()\n",
    "            y_prob.append(p)\n",
    "            y_true.append(y.cpu().numpy())\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_prob = np.concatenate(y_prob)\n",
    "    y_pred = y_prob.argmax(1)\n",
    "    report = classification_report(y_true, y_pred, target_names=classes, output_dict=True, zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    return y_true, y_prob, y_pred, report, cm\n",
    "\n",
    "def save_perclass_and_plots(tag: str, y_true, y_prob, classes: list, out_tab_dir: str, out_fig_dir: str):\n",
    "    os.makedirs(out_tab_dir, exist_ok=True); os.makedirs(out_fig_dir, exist_ok=True)\n",
    "    y_pred = y_prob.argmax(1)\n",
    "    report = classification_report(y_true, y_pred, target_names=classes, output_dict=True, zero_division=0)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv(os.path.join(out_tab_dir, f\"{tag}_perclass_metrics.csv\"))\n",
    "    try:\n",
    "        df.to_latex(os.path.join(out_tab_dir, f\"{tag}_perclass_metrics.tex\"), float_format=\"%.3f\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Confusion matrix plot\n",
    "    cmx = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    fig = plt.figure(figsize=(7,6)); plt.imshow(cmx, interpolation=\"nearest\")\n",
    "    plt.title(f\"Confusion: {tag}\"); plt.colorbar()\n",
    "    plt.xticks(range(len(classes)), classes, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(classes)), classes)\n",
    "    for i in range(cmx.shape[0]):\n",
    "        for j in range(cmx.shape[1]):\n",
    "            plt.text(j, i, str(cmx[i,j]), ha=\"center\", va=\"center\", fontsize=8)\n",
    "    plt.tight_layout(); fig.savefig(os.path.join(out_fig_dir, f\"{tag}_confmat.png\"), dpi=200); plt.close(fig)\n",
    "\n",
    "    # PR curves\n",
    "    fig = plt.figure(figsize=(7,6))\n",
    "    for c in range(len(classes)):\n",
    "        gt = (y_true==c).astype(int)\n",
    "        prec, rec, _ = precision_recall_curve(gt, y_prob[:,c])\n",
    "        ap = average_precision_score(gt, y_prob[:,c])\n",
    "        plt.plot(rec, prec, label=f\"{classes[c]} (AP={ap:.3f})\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.legend(); plt.title(f\"PR Curves: {tag}\")\n",
    "    fig.savefig(os.path.join(out_fig_dir, f\"{tag}_pr_curves.png\"), dpi=200); plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20d429a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Metrics, confusions, PR saved to .\\tables\\medmnist and .\\figs\\medmnist\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Evaluate and save artifacts for all models ---\n",
    "classes = class_names_for_medmnist()\n",
    "test_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "for tag, model in MODELS.items():\n",
    "    y_true, y_prob, y_pred, report, cmx = eval_model(model, test_loader, classes)\n",
    "    save_perclass_and_plots(tag, y_true, y_prob, classes, TABLES_ROOT, FIGS_ROOT)\n",
    "print(\"[OK] Metrics, confusions, PR saved to\", TABLES_ROOT, \"and\", FIGS_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48be915a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Ablation jsonl written under reports\\ablation_medmnist\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Single-point ablation records (alpha, tau, beta defaults for pretrained students) ---\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def ece_score_torch(probs: torch.Tensor, y: torch.Tensor, n_bins=15):\n",
    "    conf, preds = probs.max(1).values, probs.argmax(1)\n",
    "    bins = torch.linspace(0,1,n_bins+1, device=probs.device)\n",
    "    ece = torch.zeros(1, device=probs.device)\n",
    "    for i in range(n_bins):\n",
    "        m = (conf >= bins[i]) & (conf < bins[i+1])\n",
    "        if m.sum() == 0: continue\n",
    "        acc = (preds[m]==y[m]).float().mean()\n",
    "        conf_m = conf[m].mean()\n",
    "        ece += (m.float().mean()) * (conf_m - acc).abs()\n",
    "    return ece.item()\n",
    "\n",
    "ABL_DIR = Path(REPORTS_ROOT) / \"ablation_medmnist\"\n",
    "ABL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "defaults = {\n",
    "    \"resnet18\":       {\"alpha\":0.5,\"tau\":4.0,\"beta\":0.0},\n",
    "    \"mobilenet_v2\":   {\"alpha\":0.5,\"tau\":4.0,\"beta\":0.0},\n",
    "    \"efficientnet_b0\":{\"alpha\":0.5,\"tau\":4.0,\"beta\":0.0},\n",
    "}\n",
    "\n",
    "for st_tag, hp in defaults.items():\n",
    "    m = MODELS[st_tag]\n",
    "    ys=[]; ps=[]\n",
    "    with torch.inference_mode():\n",
    "        for x,y in test_loader:\n",
    "            x = x.to(DEVICE); y = y.long().to(DEVICE)\n",
    "            p = torch.softmax(m(x), dim=1).cpu()\n",
    "            ys.append(y.cpu()); ps.append(p)\n",
    "    y_true = torch.cat(ys).numpy()\n",
    "    y_prob = torch.cat(ps).numpy()\n",
    "    y_pred = y_prob.argmax(1)\n",
    "    macf1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    ece = ece_score_torch(torch.from_numpy(y_prob), torch.from_numpy(y_true))\n",
    "\n",
    "    outdir = ABL_DIR / f\"fixed_{st_tag}_medmnist\"\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    rec = {\"epoch\": 0, \"macro_f1\": float(macf1), \"ece\": float(ece),\n",
    "           \"alpha\": float(hp[\"alpha\"]), \"tau\": float(hp[\"tau\"]), \"beta\": float(hp[\"beta\"]),\n",
    "           \"student\": st_tag, \"dataset\": \"medmnist\"}\n",
    "    with open(outdir/\"metrics.jsonl\",\"w\") as f: f.write(json.dumps(rec)+\"\\n\")\n",
    "print(\"[OK] Ablation jsonl written under\", str(ABL_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0508000b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params_M</th>\n",
       "      <th>FLOPs_G</th>\n",
       "      <th>latency_ms_gpu_mean</th>\n",
       "      <th>latency_ms_cpu_mean</th>\n",
       "      <th>ram_rss_MB</th>\n",
       "      <th>gpu_peak_MB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnet50</td>\n",
       "      <td>23.522375</td>\n",
       "      <td>8.263418</td>\n",
       "      <td>8.540380</td>\n",
       "      <td>101.889646</td>\n",
       "      <td>2087.925781</td>\n",
       "      <td>424.933594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnet18</td>\n",
       "      <td>11.180103</td>\n",
       "      <td>1.823525</td>\n",
       "      <td>2.758622</td>\n",
       "      <td>49.473834</td>\n",
       "      <td>2090.476562</td>\n",
       "      <td>376.927246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mobilenet_v2</td>\n",
       "      <td>2.232839</td>\n",
       "      <td>0.326216</td>\n",
       "      <td>2.789652</td>\n",
       "      <td>13.267028</td>\n",
       "      <td>2009.359375</td>\n",
       "      <td>335.947754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>efficientnet_b0</td>\n",
       "      <td>4.016515</td>\n",
       "      <td>0.413874</td>\n",
       "      <td>5.286169</td>\n",
       "      <td>30.118823</td>\n",
       "      <td>2020.242188</td>\n",
       "      <td>342.546387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model   params_M   FLOPs_G  latency_ms_gpu_mean  \\\n",
       "0         resnet50  23.522375  8.263418             8.540380   \n",
       "1         resnet18  11.180103  1.823525             2.758622   \n",
       "2     mobilenet_v2   2.232839  0.326216             2.789652   \n",
       "3  efficientnet_b0   4.016515  0.413874             5.286169   \n",
       "\n",
       "   latency_ms_cpu_mean   ram_rss_MB  gpu_peak_MB  \n",
       "0           101.889646  2087.925781   424.933594  \n",
       "1            49.473834  2090.476562   376.927246  \n",
       "2            13.267028  2009.359375   335.947754  \n",
       "3            30.118823  2020.242188   342.546387  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Efficiency: Params / FLOPs / Latency (CPU/GPU) / RAM ---\n",
    "import copy, time, psutil, numpy as np, torch\n",
    "from types import MethodType\n",
    "\n",
    "def _params_m(model: torch.nn.Module) -> float:\n",
    "    return sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "def _strip_thop_artifacts(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Remove THOP-added hooks/attrs so forward passes don't try to touch CPU tensors on CUDA.\n",
    "    Works on the passed instance (typically a deepcopy used for timing).\n",
    "    \"\"\"\n",
    "    for m in model.modules():\n",
    "        # restore original forward if THOP wrapped it\n",
    "        for attr in (\"__original_forward__\", \"origin_forward\", \"_origin_forward\"):\n",
    "            if hasattr(m, attr):\n",
    "                try:\n",
    "                    m.forward = MethodType(getattr(m, attr), m)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        # clear forward/pre hooks\n",
    "        if hasattr(m, \"_forward_hooks\") and isinstance(m._forward_hooks, dict):\n",
    "            m._forward_hooks = {}\n",
    "        if hasattr(m, \"_forward_pre_hooks\") and isinstance(m._forward_pre_hooks, dict):\n",
    "            m._forward_pre_hooks = {}\n",
    "        if hasattr(m, \"_backward_hooks\") and isinstance(m._backward_hooks, dict):\n",
    "            m._backward_hooks = {}\n",
    "        # remove THOP counters\n",
    "        for a in (\"total_ops\", \"total_params\"):\n",
    "            if hasattr(m, a):\n",
    "                try:\n",
    "                    delattr(m, a)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return model\n",
    "\n",
    "def _flops_g_cpu(model: torch.nn.Module, imgsz=224):\n",
    "    # THOP on a CPU-only deepcopy; never re-use that instance.\n",
    "    if thop_profile is None:\n",
    "        return float(\"nan\")\n",
    "    try:\n",
    "        m_cpu = copy.deepcopy(model).cpu().eval()\n",
    "        x_cpu = torch.randn(1, 3, imgsz, imgsz)  # CPU tensor\n",
    "        macs, _ = thop_profile(m_cpu, inputs=(x_cpu,), verbose=False)\n",
    "        return macs / 1e9\n",
    "    except Exception as e:\n",
    "        print(\"[thop] warn:\", e)\n",
    "        return float(\"nan\")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def _latency_ms(model: torch.nn.Module, device: str, imgsz=224, reps=20, warmup=10):\n",
    "    # Benchmark on a fresh, cleaned copy\n",
    "    m = copy.deepcopy(model).eval()\n",
    "    _strip_thop_artifacts(m)\n",
    "    m = m.to(device)\n",
    "    x = torch.randn(1, 3, imgsz, imgsz, device=device)\n",
    "\n",
    "    # warmup\n",
    "    if device == \"cuda\":\n",
    "        for _ in range(warmup):\n",
    "            _ = m(x)\n",
    "        torch.cuda.synchronize()\n",
    "    else:\n",
    "        for _ in range(warmup):\n",
    "            _ = m(x)\n",
    "\n",
    "    # timed\n",
    "    times = []\n",
    "    for _ in range(reps):\n",
    "        t0 = time.time()\n",
    "        _ = m(x)\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        times.append((time.time() - t0) * 1000.0)\n",
    "    return float(np.mean(times)), float(np.std(times))\n",
    "\n",
    "@torch.inference_mode()\n",
    "def _gpu_peak_mb(model: torch.nn.Module, imgsz=224):\n",
    "    if not torch.cuda.is_available():\n",
    "        return float(\"nan\")\n",
    "    m = copy.deepcopy(model).eval()\n",
    "    _strip_thop_artifacts(m)\n",
    "    m = m.to(\"cuda\")\n",
    "    x = torch.randn(1, 3, imgsz, imgsz, device=\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    _ = m(x)\n",
    "    torch.cuda.synchronize()\n",
    "    peak = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n",
    "    torch.cuda.empty_cache()\n",
    "    return peak\n",
    "\n",
    "def model_stats(model: torch.nn.Module, imgsz=224, reps=20, warmup=10):\n",
    "    stats = {}\n",
    "    stats[\"params_M\"] = _params_m(model)\n",
    "    stats[\"FLOPs_G\"]  = _flops_g_cpu(model, imgsz=imgsz)\n",
    "\n",
    "    # latency\n",
    "    lat_cpu_mean, lat_cpu_std = _latency_ms(model, \"cpu\", imgsz=imgsz, reps=reps, warmup=warmup)\n",
    "    stats[\"latency_ms_cpu_mean\"] = lat_cpu_mean\n",
    "    stats[\"latency_ms_cpu_std\"]  = lat_cpu_std\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        lat_gpu_mean, lat_gpu_std = _latency_ms(model, \"cuda\", imgsz=imgsz, reps=reps, warmup=warmup)\n",
    "        stats[\"latency_ms_gpu_mean\"] = lat_gpu_mean\n",
    "        stats[\"latency_ms_gpu_std\"]  = lat_gpu_std\n",
    "        stats[\"gpu_peak_MB\"]         = _gpu_peak_mb(model, imgsz=imgsz)\n",
    "    else:\n",
    "        stats[\"latency_ms_gpu_mean\"] = float(\"nan\")\n",
    "        stats[\"latency_ms_gpu_std\"]  = float(\"nan\")\n",
    "        stats[\"gpu_peak_MB\"]         = float(\"nan\")\n",
    "\n",
    "    # process RAM (RSS)\n",
    "    stats[\"ram_rss_MB\"] = psutil.Process(os.getpid()).memory_info().rss / (1024.0 * 1024.0)\n",
    "    return stats\n",
    "\n",
    "# --- collect & save ---\n",
    "rows = []\n",
    "for tag, m in MODELS.items():\n",
    "    s = model_stats(m, imgsz=224, reps=20, warmup=10)\n",
    "    s[\"model\"] = tag\n",
    "    rows.append(s)\n",
    "\n",
    "df_eff = pd.DataFrame(rows)[[\n",
    "    \"model\",\"params_M\",\"FLOPs_G\",\"latency_ms_gpu_mean\",\"latency_ms_cpu_mean\",\n",
    "    \"ram_rss_MB\",\"gpu_peak_MB\"\n",
    "]]\n",
    "df_eff.to_csv(os.path.join(TABLES_ROOT, \"efficiency_medmnist.csv\"), index=False)\n",
    "try:\n",
    "    df_eff.to_latex(os.path.join(TABLES_ROOT, \"efficiency_medmnist.tex\"), float_format=\"%.3f\", index=False)\n",
    "except Exception:\n",
    "    pass\n",
    "df_eff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0423e263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stifl\\AppData\\Local\\Temp\\ipykernel_22228\\433166054.py:103: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  heat = (cm.get_cmap(\"jet\")(cam_up)[..., :3] * 255).astype(np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Grad-CAM panels saved to .\\figs\\medmnist\n"
     ]
    }
   ],
   "source": [
    "# --- Grad-CAM (no OpenCV) — THOP-safe ---\n",
    "import copy, os\n",
    "import numpy as np, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from types import MethodType\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "\n",
    "def _strip_thop_artifacts(model: nn.Module):\n",
    "    \"\"\"Remove THOP-added wrappers/counters, but DO NOT replace hook dict objects.\"\"\"\n",
    "    for m in model.modules():\n",
    "        # restore original forward if THOP wrapped it\n",
    "        for attr in (\"__original_forward__\", \"origin_forward\", \"_origin_forward\"):\n",
    "            if hasattr(m, attr):\n",
    "                try:\n",
    "                    m.forward = MethodType(getattr(m, attr), m)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        # clear hooks IN PLACE (do not assign a new dict)\n",
    "        if hasattr(m, \"_forward_hooks\") and m._forward_hooks is not None:\n",
    "            try: m._forward_hooks.clear()\n",
    "            except Exception: pass\n",
    "        if hasattr(m, \"_forward_pre_hooks\") and m._forward_pre_hooks is not None:\n",
    "            try: m._forward_pre_hooks.clear()\n",
    "            except Exception: pass\n",
    "        if hasattr(m, \"_backward_hooks\") and m._backward_hooks is not None:\n",
    "            try: m._backward_hooks.clear()\n",
    "            except Exception: pass\n",
    "        # remove THOP counters if present\n",
    "        for a in (\"total_ops\", \"total_params\"):\n",
    "            if hasattr(m, a):\n",
    "                try: delattr(m, a)\n",
    "                except Exception: pass\n",
    "    return model\n",
    "\n",
    "def pick_last_conv(model: nn.Module):\n",
    "    # resnet-style\n",
    "    if hasattr(model, \"layer4\"):\n",
    "        return model.layer4[-1]\n",
    "    # mobilenet/efficientnet-style\n",
    "    if hasattr(model, \"features\") and len(list(model.features.children())) > 0:\n",
    "        last = None\n",
    "        for mod in model.features.modules():\n",
    "            if isinstance(mod, nn.Conv2d):\n",
    "                last = mod\n",
    "        if last is not None:\n",
    "            return last\n",
    "        return list(model.features.children())[-1]\n",
    "    # generic fallback\n",
    "    last = None\n",
    "    for mod in model.modules():\n",
    "        if isinstance(mod, nn.Conv2d): last = mod\n",
    "    if last is None:\n",
    "        raise RuntimeError(\"No conv layer found for Grad-CAM\")\n",
    "    return last\n",
    "\n",
    "def gradcam_map(model: nn.Module, x4d: torch.Tensor, target_layer: nn.Module, class_idx: int):\n",
    "    feats, grads = {}, {}\n",
    "\n",
    "    def fwd_hook(m, i, o): feats[\"z\"] = o\n",
    "    def bwd_hook(m, gi, go): grads[\"g\"] = go[0]\n",
    "\n",
    "    # register hooks (works because we didn't replace the dict objects)\n",
    "    h1 = target_layer.register_forward_hook(fwd_hook)\n",
    "    h2 = target_layer.register_full_backward_hook(bwd_hook)\n",
    "\n",
    "    # ensure grad tracking is on\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(True)\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        logits = model(x4d)\n",
    "        C = logits.shape[1]\n",
    "        cls = int(max(0, min(class_idx, C - 1)))\n",
    "        score = logits[:, cls].sum()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        score.backward(retain_graph=False)\n",
    "\n",
    "    A = feats[\"z\"].detach()          # [N,K,H',W']\n",
    "    G = grads[\"g\"].detach()          # [N,K,H',W']\n",
    "    w = G.flatten(2).mean(2)         # [N,K]\n",
    "    cam = (w[:, :, None, None] * A).sum(1, keepdim=True)  # [N,1,H',W']\n",
    "    cam = torch.relu(cam)\n",
    "    cam = cam / (cam.amax(dim=(2, 3), keepdim=True) + 1e-6)\n",
    "\n",
    "    h1.remove(); h2.remove()\n",
    "    model.train(was_training)\n",
    "    return cam  # [N,1,H',W']\n",
    "\n",
    "def denorm_to_uint8(x3: torch.Tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=x3.device)[:, None, None]\n",
    "    std  = torch.tensor([0.229, 0.224, 0.225], device=x3.device)[:, None, None]\n",
    "    img = (x3*std + mean).clamp(0, 1).permute(1, 2, 0).contiguous().detach().cpu().numpy()\n",
    "    return (img*255).astype(np.uint8)\n",
    "\n",
    "def overlay_heatmap(rgb_u8: np.ndarray, cam_01: np.ndarray, alpha=0.35):\n",
    "    H, W = rgb_u8.shape[:2]\n",
    "    cam_t = torch.from_numpy(cam_01)[None, None, ...].float()\n",
    "    cam_up = F.interpolate(cam_t, size=(H, W), mode=\"bilinear\", align_corners=False)[0, 0].numpy()\n",
    "    heat = (cm.get_cmap(\"jet\")(cam_up)[..., :3] * 255).astype(np.uint8)\n",
    "    out = (1 - alpha) * rgb_u8 + alpha * heat\n",
    "    return np.clip(out, 0, 255).astype(np.uint8)\n",
    "\n",
    "# --- Build class-balanced index list (graceful if a class has <K examples) ---\n",
    "K = 8\n",
    "classes = class_names_for_medmnist()\n",
    "idxs_per_class = {c: [] for c in range(len(classes))}\n",
    "for i in range(len(testset)):\n",
    "    _, y = testset[i]\n",
    "    y = int(np.array(y).squeeze())\n",
    "    if 0 <= y < len(classes) and len(idxs_per_class[y]) < K:\n",
    "        idxs_per_class[y].append(i)\n",
    "    if all(len(v) >= K for v in idxs_per_class.values()):\n",
    "        break\n",
    "\n",
    "# --- Generate panels per model on a cleaned copy (CPU or CUDA) ---\n",
    "os.makedirs(FIGS_ROOT, exist_ok=True)\n",
    "for tag, model in MODELS.items():\n",
    "    m = copy.deepcopy(model).eval()\n",
    "    _strip_thop_artifacts(m)   # critical: keep hook dict objects, just clear\n",
    "    m = m.to(DEVICE)\n",
    "    target_layer = pick_last_conv(m)\n",
    "\n",
    "    rows = []\n",
    "    for c in range(len(classes)):\n",
    "        tiles = []\n",
    "        for idx in idxs_per_class[c]:\n",
    "            x, _ = testset[idx]\n",
    "            x = x.unsqueeze(0).to(DEVICE)\n",
    "            with torch.enable_grad():\n",
    "                cam = gradcam_map(m, x, target_layer, c)[0, 0].detach().cpu().numpy()\n",
    "            rgb = denorm_to_uint8(x[0])\n",
    "            tiles.append(overlay_heatmap(rgb, cam))\n",
    "        if len(tiles) == 0:\n",
    "            continue\n",
    "        rows.append(np.concatenate(tiles, axis=1))\n",
    "    if len(rows) == 0:\n",
    "        print(f\"[Grad-CAM] No rows for {tag}; skipping.\")\n",
    "        continue\n",
    "    maxW = max(r.shape[1] for r in rows)\n",
    "    rows_pad = []\n",
    "    for r in rows:\n",
    "        if r.shape[1] < maxW:\n",
    "            pad = np.zeros((r.shape[0], maxW - r.shape[1], 3), dtype=r.dtype)\n",
    "            r = np.concatenate([r, pad], axis=1)\n",
    "        rows_pad.append(r)\n",
    "    panel = np.concatenate(rows_pad, axis=0)\n",
    "    Image.fromarray(panel).save(os.path.join(FIGS_ROOT, f\"{tag}_gradcam_panel.png\"))\n",
    "print(\"[OK] Grad-CAM panels saved to\", FIGS_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f57ab4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] t-SNE plots saved to .\\figs\\medmnist\n"
     ]
    }
   ],
   "source": [
    "# --- t-SNE (teacher vs student) — THOP-safe, device-clean ---\n",
    "import copy, os\n",
    "import numpy as np, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _clean_clone(model: nn.Module, device: str):\n",
    "    m = copy.deepcopy(model).eval()\n",
    "    _strip_thop_artifacts(m)      # from your Grad-CAM cell\n",
    "    return m.to(device)\n",
    "\n",
    "def penultimate_features(model: nn.Module, x4d: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Works for torchvision resnet*, mobilenet_v2, efficientnet_b0.\n",
    "    \"\"\"\n",
    "    # ResNet family\n",
    "    if hasattr(model, \"layer4\") and hasattr(model, \"avgpool\"):\n",
    "        x = model.conv1(x4d); x = model.bn1(x); x = model.relu(x); x = model.maxpool(x)\n",
    "        x = model.layer1(x); x = model.layer2(x); x = model.layer3(x); x = model.layer4(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).flatten(1)\n",
    "        return x\n",
    "    # MobileNetV2 / EfficientNet style (features -> GAP)\n",
    "    if hasattr(model, \"features\"):\n",
    "        x = model.features(x4d)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).flatten(1)\n",
    "        return x\n",
    "    # Fallback: last conv then GAP\n",
    "    last = None\n",
    "    for mod in model.modules():\n",
    "        if isinstance(mod, nn.Conv2d): last = mod\n",
    "    if last is None:\n",
    "        raise RuntimeError(\"No conv block found for penultimate features\")\n",
    "    feats = {}\n",
    "    def hk(_, __, o): feats[\"z\"] = o\n",
    "    h = last.register_forward_hook(hk)\n",
    "    _ = model(x4d)\n",
    "    h.remove()\n",
    "    x = feats[\"z\"]\n",
    "    x = F.adaptive_avg_pool2d(x, 1).flatten(1)\n",
    "    return x\n",
    "\n",
    "@torch.inference_mode()\n",
    "def collect_feats(model: nn.Module, loader: torch.utils.data.DataLoader, limit=1500):\n",
    "    m = _clean_clone(model, DEVICE)   # <-- critical: deep copy + strip THOP + move to DEVICE\n",
    "    Z, Y, n = [], [], 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(DEVICE)\n",
    "        z = penultimate_features(m, x).cpu().numpy()\n",
    "        Z.append(z); Y.append(y.numpy()); n += len(y)\n",
    "        if n >= limit:\n",
    "            break\n",
    "    Z = np.vstack(Z)[:limit]; Y = np.concatenate(Y)[:limit]\n",
    "    return Z, Y\n",
    "\n",
    "def procrustes_align(A, B):\n",
    "    # Align A to B (orthogonal)\n",
    "    A0 = A - A.mean(0); B0 = B - B.mean(0)\n",
    "    U, _, Vt = np.linalg.svd(A0.T @ B0)\n",
    "    R = U @ Vt\n",
    "    return (A0 @ R) + B.mean(0)\n",
    "\n",
    "# smaller loader to avoid pulling full test at once\n",
    "small_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Teacher embedding\n",
    "Zt, Yt = collect_feats(teacher, small_loader, limit=1500)\n",
    "T2 = TSNE(n_components=2, init=\"pca\", learning_rate=\"auto\", perplexity=30, random_state=0).fit_transform(Zt)\n",
    "\n",
    "# Students\n",
    "for st in [\"resnet18\",\"mobilenet_v2\",\"efficientnet_b0\"]:\n",
    "    Zs, Ys = collect_feats(MODELS[st], small_loader, limit=1500)\n",
    "    S2 = TSNE(n_components=2, init=\"pca\", learning_rate=\"auto\", perplexity=30, random_state=0).fit_transform(Zs)\n",
    "    S2a = procrustes_align(S2, T2)\n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    plt.scatter(T2[:,0], T2[:,1], s=6, alpha=0.35, label=\"resnet50\")\n",
    "    plt.scatter(S2a[:,0], S2a[:,1], s=6, alpha=0.35, label=st)\n",
    "    plt.title(f\"t-SNE (DermaMNIST) — teacher vs {st}\")\n",
    "    plt.legend()\n",
    "    fig.savefig(os.path.join(FIGS_ROOT, f\"tsne_teacher_vs_{st}.png\"), dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "print(\"[OK] t-SNE plots saved to\", FIGS_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550ea71d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
